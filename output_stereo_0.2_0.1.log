/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
/home/alonkay/Thesis/FMatrixRegressor.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
###########################################################################################################################################################

                         learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
                        batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
                        average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, part: head, get_old_path: False,
                        RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: plots/Affine/BS_32__lr_6e-05__train_size_9216__CLIP__alpha_10__conv__original_rotated/model.pth,
                        crop: 224 resize: 256, use conv: False pretrained: None, train_size: 0.2, norm_mean: tensor([0.4815, 0.4578, 0.4082], device='cuda:0'), norm_std: tensor([0.2686, 0.2613, 0.2758], device='cuda:0'), sched: None seed: 42, 


##### CONTINUE TRAINING #####



################
Empty points at ('00', '05', '02', '02', '05', '00', '02', '02')


Epoch 1491/3000: Training Loss: 0.8593461152812212		 Val Loss: 1.1997272657311482
             	Training MAE: 0.36105877161026		 Val MAE: 0.3617362082004547
             	Algebraic dist: 0.12045258525552785		 Val Algebraic dist: 0.17898536765057108
             	RE1 dist: 0.01349427867199662		 Val RE1 dist: 0.02563116861426312
             	SED dist: 0.6990314216191479		 Val SED dist: 1.3785150776738706



################
Empty points at ('02', '00', '02', '05', '02', '00', '02', '00')


Epoch 1492/3000: Training Loss: 0.7994732733582218		 Val Loss: 0.8894751175590183
             	Training MAE: 0.3597838282585144		 Val MAE: 0.367840439081192
             	Algebraic dist: 0.10727658570912492		 Val Algebraic dist: 0.12608281425807788
             	RE1 dist: 0.01088988561032003		 Val RE1 dist: 0.015099459368249645
             	SED dist: 0.5864212768104243		 Val SED dist: 0.7108748892079229


Epoch 1493/3000: Training Loss: 1.3860972499495503		 Val Loss: 1.5899747765582541
             	Training MAE: 0.36253705620765686		 Val MAE: 0.3615735173225403
             	Algebraic dist: 0.19614031745938798		 Val Algebraic dist: 0.1907894922339398
             	RE1 dist: 0.03554783360104719		 Val RE1 dist: 0.030775907246962837
             	SED dist: 1.7415059782922049		 Val SED dist: 2.157610851785411


Epoch 1494/3000: Training Loss: 0.9739813435121656		 Val Loss: 0.9646110534667969
             	Training MAE: 0.36203649640083313		 Val MAE: 0.3700776696205139
             	Algebraic dist: 0.1427606814901767		 Val Algebraic dist: 0.15644771119822626
             	RE1 dist: 0.017952626921593922		 Val RE1 dist: 0.020627019198044487
             	SED dist: 0.9202829621374827		 Val SED dist: 0.8487688147503397



################
Empty points at ('02', '02', '02', '02', '02', '02', '05', '05')


Epoch 1495/3000: Training Loss: 0.9512794184948685		 Val Loss: 1.7488341953443445
             	Training MAE: 0.36127716302871704		 Val MAE: 0.3688220679759979
             	Algebraic dist: 0.13698076761956585		 Val Algebraic dist: 0.2461771757706352
             	RE1 dist: 0.01664531010983175		 Val RE1 dist: 0.04199000545169996
             	SED dist: 0.8820117795599343		 Val SED dist: 2.4330779365871265



################
Empty points at ('02', '02', '05', '00', '00', '00', '02', '02')


Epoch 1496/3000: Training Loss: 0.8141995813573858		 Val Loss: 1.147793147874915
             	Training MAE: 0.3610999882221222		 Val MAE: 0.37385115027427673
             	Algebraic dist: 0.11264681376214397		 Val Algebraic dist: 0.17385455836420474
             	RE1 dist: 0.011591575242496505		 Val RE1 dist: 0.026809954124948254
             	SED dist: 0.60895422903814		 Val SED dist: 1.1874918730362602


Epoch 1497/3000: Training Loss: 0.8461124096409421		 Val Loss: 0.997330789980681
             	Training MAE: 0.3607943654060364		 Val MAE: 0.3675876259803772
             	Algebraic dist: 0.11360241654174354		 Val Algebraic dist: 0.13725999127263608
             	RE1 dist: 0.012870434905330193		 Val RE1 dist: 0.01715503049933392
             	SED dist: 0.6739175381255765		 Val SED dist: 0.9307860498842986


Epoch 1498/3000: Training Loss: 0.9378269660076972		 Val Loss: 1.5007140118142832
             	Training MAE: 0.36135968565940857		 Val MAE: 0.3621305823326111
             	Algebraic dist: 0.13586386367403713		 Val Algebraic dist: 0.18072986602783203
             	RE1 dist: 0.01647568980706134		 Val RE1 dist: 0.025423049926757812
             	SED dist: 0.8551969633771045		 Val SED dist: 1.9762870125148608


Epoch 1499/3000: Training Loss: 1.502640389868254		 Val Loss: 1.0804508043372112
             	Training MAE: 0.3639523386955261		 Val MAE: 0.3709256649017334
             	Algebraic dist: 0.1987556373061289		 Val Algebraic dist: 0.1651215242302936
             	RE1 dist: 0.03932562613399267		 Val RE1 dist: 0.027003404886826225
             	SED dist: 1.9684567609836254		 Val SED dist: 1.0702188740605894


Epoch 1500/3000: Training Loss: 0.8630553523552814		 Val Loss: 0.9045682990032694
             	Training MAE: 0.36137866973876953		 Val MAE: 0.36906683444976807
             	Algebraic dist: 0.12074354389936721		 Val Algebraic dist: 0.13871065430019214
             	RE1 dist: 0.013700033905761268		 Val RE1 dist: 0.016772697801175324
             	SED dist: 0.7036889558348709		 Val SED dist: 0.736237733260445


Epoch 1501/3000: Training Loss: 0.8277814238713677		 Val Loss: 1.305627905804178
             	Training MAE: 0.3616906702518463		 Val MAE: 0.36660999059677124
             	Algebraic dist: 0.11652049103346258		 Val Algebraic dist: 0.1778927471326745
             	RE1 dist: 0.012589780167020115		 Val RE1 dist: 0.025867169317991837
             	SED dist: 0.6324996666714714		 Val SED dist: 1.5536671514096467


Epoch 1502/3000: Training Loss: 0.7393517300651522		 Val Loss: 1.0456149059793223
             	Training MAE: 0.3606395721435547		 Val MAE: 0.36458295583724976
             	Algebraic dist: 0.09976567201508807		 Val Algebraic dist: 0.13720481292061185
             	RE1 dist: 0.008916601483672307		 Val RE1 dist: 0.01781349337619284
             	SED dist: 0.4617209346531942		 Val SED dist: 1.0474667756453804



################
Empty points at ('02', '03', '02', '02', '00', '05', '02', '05')


Epoch 1503/3000: Training Loss: 0.7642474297667782		 Val Loss: 5.682041334069294
             	Training MAE: 0.359754741191864		 Val MAE: 0.37541958689689636
             	Algebraic dist: 0.10190437992560468		 Val Algebraic dist: 0.6728233669115149
             	RE1 dist: 0.009972285520546551		 Val RE1 dist: 0.3001364210377569
             	SED dist: 0.5155233122765798		 Val SED dist: 10.239228621773098



################
Empty points at ('05', '02', '00', '02', '02', '02', '00', '00')


Epoch 1504/3000: Training Loss: 1.9270253762107934		 Val Loss: 0.9246108013650646
             	Training MAE: 0.36375030875205994		 Val MAE: 0.3672301769256592
             	Algebraic dist: 0.2119115850582334		 Val Algebraic dist: 0.13476899395818295
             	RE1 dist: 0.05150682723830107		 Val RE1 dist: 0.015622080668159153
             	SED dist: 2.8166377781941883		 Val SED dist: 0.7883229462996774



################
Empty points at ('00', '02', '02', '00', '03', '05', '05', '02')


Epoch 1505/3000: Training Loss: 0.7603238939799066		 Val Loss: 0.9835654548976732
             	Training MAE: 0.35969823598861694		 Val MAE: 0.36791881918907166
             	Algebraic dist: 0.10122497671204739		 Val Algebraic dist: 0.1299249503923499
             	RE1 dist: 0.00952100401874838		 Val RE1 dist: 0.017800714658654255
             	SED dist: 0.507246348250836		 Val SED dist: 0.8943128171174423


Epoch 1506/3000: Training Loss: 0.9091535054449665		 Val Loss: 1.1289896757706352
             	Training MAE: 0.3612968623638153		 Val MAE: 0.3688918352127075
             	Algebraic dist: 0.12810046206541167		 Val Algebraic dist: 0.1857120057810908
             	RE1 dist: 0.01585554108848431		 Val RE1 dist: 0.028105860171110733
             	SED dist: 0.7965539460692459		 Val SED dist: 1.1870669489321501


Epoch 1507/3000: Training Loss: 0.8801484055184791		 Val Loss: 1.2219889267631199
             	Training MAE: 0.361031174659729		 Val MAE: 0.36845478415489197
             	Algebraic dist: 0.12460158231953414		 Val Algebraic dist: 0.13314424390378204
             	RE1 dist: 0.014182597508729603		 Val RE1 dist: 0.02280072543932044
             	SED dist: 0.7399196272846518		 Val SED dist: 1.3695218459419582


Epoch 1508/3000: Training Loss: 0.8444775796024562		 Val Loss: 1.2268243872601052
             	Training MAE: 0.36067402362823486		 Val MAE: 0.37662601470947266
             	Algebraic dist: 0.1186862537341804		 Val Algebraic dist: 0.18687441038048785
             	RE1 dist: 0.01296197356333152		 Val RE1 dist: 0.03435611984004145
             	SED dist: 0.6710142015970941		 Val SED dist: 1.324415870334791


Epoch 1509/3000: Training Loss: 1.241540549865948		 Val Loss: 1.3509932808254077
             	Training MAE: 0.3619464039802551		 Val MAE: 0.36071035265922546
             	Algebraic dist: 0.1744359428152387		 Val Algebraic dist: 0.17081574771715247
             	RE1 dist: 0.027648408474517484		 Val RE1 dist: 0.023316417051398235
             	SED dist: 1.4554153949132265		 Val SED dist: 1.6812828727390454


Epoch 1510/3000: Training Loss: 1.177138662866121		 Val Loss: 3.0773060010827105
             	Training MAE: 0.3616512417793274		 Val MAE: 0.3721254765987396
             	Algebraic dist: 0.1593358930186592		 Val Algebraic dist: 0.32192794136379077
             	RE1 dist: 0.02521881230202988		 Val RE1 dist: 0.08396030508953592
             	SED dist: 1.3311464654563538		 Val SED dist: 5.061703225840693


Epoch 1511/3000: Training Loss: 0.9107272440217078		 Val Loss: 0.9702780350394871
             	Training MAE: 0.3616868555545807		 Val MAE: 0.36544832587242126
             	Algebraic dist: 0.1186060324806129		 Val Algebraic dist: 0.14672022280485733
             	RE1 dist: 0.015630197700979086		 Val RE1 dist: 0.019460027632505997
             	SED dist: 0.7978755486407403		 Val SED dist: 0.8916351484215778


Fatal Python error: Illegal instruction

Thread 0x000077f0a6a006c0 (most recent call first):
<no Python frame>

Current thread 0x000077f24ef3e580 (most recent call first):
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_misc.py", line 89 in _get_gaussian_kernel1d
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_misc.py", line 96 in _get_gaussian_kernel2d
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_misc.py", line 147 in gaussian_blur_image
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py", line 35 in _call_kernel
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/_misc.py", line 205 in _transform
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py", line 51 in <listcomp>
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py", line 50 in forward
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562 in _call_impl
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553 in _wrapped_call_impl
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py", line 51 in forward
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562 in _call_impl
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553 in _wrapped_call_impl
  File "/home/alonkay/Thesis/Dataset.py", line 99 in __getitem__
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/utils/data/dataset.py", line 350 in __getitem__
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52 in <listcomp>
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52 in fetch
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 673 in _next_data
  File "/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630 in __next__
  File "/home/alonkay/Thesis/FMatrixRegressor.py", line 244 in dataloader_step
  File "/home/alonkay/Thesis/FMatrixRegressor.py", line 185 in train_model
  File "/home/alonkay/Thesis/Main.py", line 96 in <module>
