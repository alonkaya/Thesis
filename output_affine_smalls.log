/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
/home/alonkay/Thesis/FMatrixRegressor.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
###########################################################################################################################################################

                         learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
                        batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a'], dataset: Stereo,
                        average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, part: head, get_old_path: False,
                        RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: plots/Affine/BS_32__lr_6e-05__train_size_9216__CLIP__alpha_10__conv__original_rotated/model.pth,
                        crop: 224 resize: 256, use conv: False pretrained: None, train_size: 0.025, norm_mean: tensor([0.4815, 0.4578, 0.4082], device='cuda:0'), norm_std: tensor([0.2686, 0.2613, 0.2758], device='cuda:0'), sched: None seed: 42, 


##### CONTINUE TRAINING #####


Epoch 9782/12000: Training Loss: 0.34670142566456513		 Val Loss: 0.9017342726389567
             	Training MAE: 0.093475840985775		 Val MAE: 0.08727628737688065
             	Algebraic dist: 0.17792656842400045		 Val Algebraic dist: 0.21644218762715658
             	RE1 dist: 0.025610608213088092		 Val RE1 dist: 0.05563962459564209
             	SED dist: 0.6246751336490407		 Val SED dist: 1.7459152539571126


Epoch 9783/12000: Training Loss: 0.23398944910834818		 Val Loss: 0.7866725126902262
             	Training MAE: 0.09396317601203918		 Val MAE: 0.09043681621551514
             	Algebraic dist: 0.13112592697143555		 Val Algebraic dist: 0.21896511316299438
             	RE1 dist: 0.016224145889282227		 Val RE1 dist: 0.05831150710582733
             	SED dist: 0.39814059874590707		 Val SED dist: 1.5127398173014324


Epoch 9784/12000: Training Loss: 2.1281291737275967		 Val Loss: 1.2102182706197102
             	Training MAE: 0.11126720160245895		 Val MAE: 0.12711220979690552
             	Algebraic dist: 0.38848161697387695		 Val Algebraic dist: 0.25548909107844037
             	RE1 dist: 0.13826344994937673		 Val RE1 dist: 0.05271753668785095
             	SED dist: 4.1660290886374085		 Val SED dist: 2.3119918505350747


Epoch 9785/12000: Training Loss: 0.4298594138201545		 Val Loss: 1.0930769443511963
             	Training MAE: 0.10261233150959015		 Val MAE: 0.0850398913025856
             	Algebraic dist: 0.18617750616634593		 Val Algebraic dist: 0.2253734270731608
             	RE1 dist: 0.028375068131615135		 Val RE1 dist: 0.06678342322508495
             	SED dist: 0.7792899187873391		 Val SED dist: 2.1301172574361167


Epoch 9786/12000: Training Loss: 0.5381149404189166		 Val Loss: 2.4632665316263833
             	Training MAE: 0.10406960546970367		 Val MAE: 0.08865748345851898
             	Algebraic dist: 0.19639277458190918		 Val Algebraic dist: 0.34337810675303143
             	RE1 dist: 0.03256299565820133		 Val RE1 dist: 0.1040952702363332
             	SED dist: 0.9932627958409926		 Val SED dist: 4.866392453511556


Epoch 9787/12000: Training Loss: 0.38955385544720816		 Val Loss: 0.9599717458089193
             	Training MAE: 0.1000681072473526		 Val MAE: 0.1009581908583641
             	Algebraic dist: 0.1662754311281092		 Val Algebraic dist: 0.26872358719507855
             	RE1 dist: 0.025155603885650635		 Val RE1 dist: 0.06192615131537119
             	SED dist: 0.7034719691557043		 Val SED dist: 1.8482505480448406


Epoch 9788/12000: Training Loss: 0.2734491684857537		 Val Loss: 0.8183406194051107
             	Training MAE: 0.09506036341190338		 Val MAE: 0.0859881266951561
             	Algebraic dist: 0.1438887119293213		 Val Algebraic dist: 0.22208666801452637
             	RE1 dist: 0.01741375642664292		 Val RE1 dist: 0.05208954711755117
             	SED dist: 0.47556888355928306		 Val SED dist: 1.580105145772298


Epoch 9789/12000: Training Loss: 0.421048641204834		 Val Loss: 1.6565098762512207
             	Training MAE: 0.09506626427173615		 Val MAE: 0.0876011773943901
             	Algebraic dist: 0.19374886681051814		 Val Algebraic dist: 0.26834650834401447
             	RE1 dist: 0.029636544339797077		 Val RE1 dist: 0.07416905959447224
             	SED dist: 0.7717290205114028		 Val SED dist: 3.2536773681640625


Epoch 9790/12000: Training Loss: 0.4114219721625833		 Val Loss: 1.2982311248779297
             	Training MAE: 0.10021162033081055		 Val MAE: 0.10347700864076614
             	Algebraic dist: 0.17454837350284352		 Val Algebraic dist: 0.2581384778022766
             	RE1 dist: 0.026249620844336116		 Val RE1 dist: 0.061233063538869224
             	SED dist: 0.7459209105547737		 Val SED dist: 2.5224477450052896


Epoch 9791/12000: Training Loss: 0.2139864949619069		 Val Loss: 0.7232053279876709
             	Training MAE: 0.0960860475897789		 Val MAE: 0.08803299814462662
             	Algebraic dist: 0.1308564578785616		 Val Algebraic dist: 0.2314876119295756
             	RE1 dist: 0.014700663440367755		 Val RE1 dist: 0.05309107402960459
             	SED dist: 0.35669388490564685		 Val SED dist: 1.3883428573608398


Epoch 9792/12000: Training Loss: 0.4582553751328412		 Val Loss: 1.1533034642537434
             	Training MAE: 0.09743520617485046		 Val MAE: 0.09795879572629929
             	Algebraic dist: 0.18897616162019618		 Val Algebraic dist: 0.22091897328694662
             	RE1 dist: 0.032417977557462806		 Val RE1 dist: 0.061802640557289124
             	SED dist: 0.8423766528858858		 Val SED dist: 2.238917350769043


Epoch 9793/12000: Training Loss: 0.31748098485610066		 Val Loss: 0.9792826970418295
             	Training MAE: 0.09936069697141647		 Val MAE: 0.08561433106660843
             	Algebraic dist: 0.15295822480145624		 Val Algebraic dist: 0.2382753094037374
             	RE1 dist: 0.020282096722546744		 Val RE1 dist: 0.060073827703793846
             	SED dist: 0.5592351240270278		 Val SED dist: 1.9022061030069988


Epoch 9794/12000: Training Loss: 1.555112502154182		 Val Loss: 4.138099034627278
             	Training MAE: 0.11550027132034302		 Val MAE: 0.14593525230884552
             	Algebraic dist: 0.33260642780977134		 Val Algebraic dist: 0.521735429763794
             	RE1 dist: 0.0935891586191514		 Val RE1 dist: 0.1931880315144857
             	SED dist: 3.01153564453125		 Val SED dist: 8.130443572998047


Epoch 9795/12000: Training Loss: 0.5948144688325769		 Val Loss: 1.1215449968973796
             	Training MAE: 0.10368192195892334		 Val MAE: 0.08901012688875198
             	Algebraic dist: 0.2151794714086196		 Val Algebraic dist: 0.28943753242492676
             	RE1 dist: 0.03990390370873844		 Val RE1 dist: 0.07187157869338989
             	SED dist: 1.108722350176643		 Val SED dist: 2.1834278106689453


Epoch 9796/12000: Training Loss: 0.5218027900247013		 Val Loss: 1.083005428314209
             	Training MAE: 0.09663793444633484		 Val MAE: 0.09760483354330063
             	Algebraic dist: 0.17835302913890166		 Val Algebraic dist: 0.26229466994603473
             	RE1 dist: 0.03617270203197703		 Val RE1 dist: 0.06230294704437256
             	SED dist: 0.9716539943919462		 Val SED dist: 2.098563035329183


Epoch 9797/12000: Training Loss: 0.6349377912633559		 Val Loss: 1.1555182139078777
             	Training MAE: 0.0986134260892868		 Val MAE: 0.09770482778549194
             	Algebraic dist: 0.22004933918223663		 Val Algebraic dist: 0.28936747709910077
             	RE1 dist: 0.04108406165066887		 Val RE1 dist: 0.07254144549369812
             	SED dist: 1.1950491736916935		 Val SED dist: 2.2440508206685386


Epoch 9798/12000: Training Loss: 0.38063857134650736		 Val Loss: 0.8209396203358968
             	Training MAE: 0.09542431682348251		 Val MAE: 0.08465194702148438
             	Algebraic dist: 0.17386747809017405		 Val Algebraic dist: 0.2060405413309733
             	RE1 dist: 0.025689309134202844		 Val RE1 dist: 0.04763350387414297
             	SED dist: 0.6893649942734662		 Val SED dist: 1.5854814847310383


Epoch 9799/12000: Training Loss: 0.36506930519552794		 Val Loss: 3.7553650538126626
             	Training MAE: 0.09457946568727493		 Val MAE: 0.0997849553823471
             	Algebraic dist: 0.16256427764892578		 Val Algebraic dist: 0.5939453840255737
             	RE1 dist: 0.02574817047399633		 Val RE1 dist: 0.22698533535003662
             	SED dist: 0.6600612752577838		 Val SED dist: 7.441335042317708


Epoch 9800/12000: Training Loss: 0.6762004740097943		 Val Loss: 0.9994726181030273
             	Training MAE: 0.0966760665178299		 Val MAE: 0.08830699324607849
             	Algebraic dist: 0.23144341917598948		 Val Algebraic dist: 0.21626738707224527
             	RE1 dist: 0.047261644812191236		 Val RE1 dist: 0.05133078992366791
             	SED dist: 1.2801642698400162		 Val SED dist: 1.9393994013468425


Epoch 9801/12000: Training Loss: 0.4210663122289321		 Val Loss: 1.0917518138885498
             	Training MAE: 0.09482043236494064		 Val MAE: 0.11871368438005447
             	Algebraic dist: 0.19290766996495864		 Val Algebraic dist: 0.23625163237253824
             	RE1 dist: 0.029280352241852704		 Val RE1 dist: 0.04988169173399607
             	SED dist: 0.7711967580458697		 Val SED dist: 2.0884591738382974


Epoch 9802/12000: Training Loss: 0.31037254894480987		 Val Loss: 1.4030601183573406
             	Training MAE: 0.09881142526865005		 Val MAE: 0.08756688982248306
             	Algebraic dist: 0.1526928789475385		 Val Algebraic dist: 0.2748577992121379
             	RE1 dist: 0.021023513639674467		 Val RE1 dist: 0.0659025510152181
             	SED dist: 0.5447696797987994		 Val SED dist: 2.7473007837931314


Epoch 9803/12000: Training Loss: 0.35503771725822897		 Val Loss: 0.8056045373280843
             	Training MAE: 0.09637635201215744		 Val MAE: 0.08710458874702454
             	Algebraic dist: 0.16026470240424662		 Val Algebraic dist: 0.1869054635365804
             	RE1 dist: 0.023299366235733032		 Val RE1 dist: 0.05345944066842397
             	SED dist: 0.638827548307531		 Val SED dist: 1.5538921356201172


Epoch 9804/12000: Training Loss: 0.3563803784987506		 Val Loss: 0.730858325958252
             	Training MAE: 0.09549098461866379		 Val MAE: 0.0981585904955864
             	Algebraic dist: 0.17123815592597513		 Val Algebraic dist: 0.22296792268753052
             	RE1 dist: 0.025868843583499685		 Val RE1 dist: 0.04812435805797577
             	SED dist: 0.641817429486443		 Val SED dist: 1.3938398361206055


Epoch 9805/12000: Training Loss: 0.43715541503008676		 Val Loss: 1.3770179748535156
             	Training MAE: 0.09458404779434204		 Val MAE: 0.10225070267915726
             	Algebraic dist: 0.20082476559807272		 Val Algebraic dist: 0.3667310873667399
             	RE1 dist: 0.0324702543370864		 Val RE1 dist: 0.09234009186426799
             	SED dist: 0.8058233261108398		 Val SED dist: 2.681010882059733


Epoch 9806/12000: Training Loss: 0.5336391224580652		 Val Loss: 0.9370490709940592
             	Training MAE: 0.09905073046684265		 Val MAE: 0.08616470545530319
             	Algebraic dist: 0.20563331772299373		 Val Algebraic dist: 0.2141749064127604
             	RE1 dist: 0.036164764095755184		 Val RE1 dist: 0.05148945748806
             	SED dist: 0.9910246905158547		 Val SED dist: 1.816426436106364


