Sat 07 Dec 2024 18:17:31 IST

SLURM_JOBID:		 1783461
SLURM_JOB_NODELIST:	 cs-4090-02 


/home/alonkay/.conda/envs/alon_env/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/.conda/envs/alon_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/cs_storage/alonkay/Thesis/FMatrixRegressor.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')

###########################################################################################################################################################

__fresh_MLP learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Kitti2Flying,
average embeddings: False, model: microsoft/resnet-152, augmentation: True, random crop: True, part: head, get_old_path: False,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 150, norm_mean: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), norm_std: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), sched: None seed: 42, 

train size: 1472, val size: 361, test size: 968

##### CONTINUE TRAINING #####

Epoch 5430/10000: Training Loss: 0.9172507576320482		 Val Loss: 2.1890811090884
             	Training MAE: 0.07118165493011475		 Val MAE: 0.06006304547190666
             	Algebraic dist: 0.29771786150724994		 Val Algebraic dist: 0.37037364296291186
             	RE1 dist: 0.09846364933511485		 Val RE1 dist: 0.2596861590509829
             	SED dist: 1.7677074929942256		 Val SED dist: 4.340975222380265

Epoch 5431/10000: Training Loss: 0.6251845981763757		 Val Loss: 1.9532858807107676
             	Training MAE: 0.06968344748020172		 Val MAE: 0.06595177203416824
             	Algebraic dist: 0.25082751978998596		 Val Algebraic dist: 0.33229927394701086
             	RE1 dist: 0.06640244048574696		 Val RE1 dist: 0.23011931129123853
             	SED dist: 1.1844633351201597		 Val SED dist: 3.8596761952275815

Epoch 5432/10000: Training Loss: 0.8505676932956862		 Val Loss: 2.1704708596934443
             	Training MAE: 0.07053259760141373		 Val MAE: 0.06134692579507828
             	Algebraic dist: 0.26856362301370373		 Val Algebraic dist: 0.3567900450333305
             	RE1 dist: 0.08411138990651006		 Val RE1 dist: 0.24943554919698965
             	SED dist: 1.6345420505689539		 Val SED dist: 4.299395022184952

Epoch 5433/10000: Training Loss: 0.2825837757276452		 Val Loss: 1.77069207896357
             	Training MAE: 0.06739597022533417		 Val MAE: 0.052297964692115784
             	Algebraic dist: 0.16474414908367654		 Val Algebraic dist: 0.37426160729449726
             	RE1 dist: 0.02938016342080158		 Val RE1 dist: 0.2124559153681216
             	SED dist: 0.502158662547236		 Val SED dist: 3.5115386299465015

Epoch 5434/10000: Training Loss: 0.26848434365313983		 Val Loss: 1.6302034129267153
             	Training MAE: 0.06774859875440598		 Val MAE: 0.051157817244529724
             	Algebraic dist: 0.16304323984229047		 Val Algebraic dist: 0.34091319208559784
             	RE1 dist: 0.028266764205435047		 Val RE1 dist: 0.20541101953257684
             	SED dist: 0.474054129227348		 Val SED dist: 3.2323492298955503

Epoch 5435/10000: Training Loss: 0.8899127296779467		 Val Loss: 2.1868115300717563
             	Training MAE: 0.07288652658462524		 Val MAE: 0.06253968924283981
             	Algebraic dist: 0.29541025991025177		 Val Algebraic dist: 0.3650104688561481
             	RE1 dist: 0.09391193804533585		 Val RE1 dist: 0.25448994014574133
             	SED dist: 1.7104654726774797		 Val SED dist: 4.333693794582201

Epoch 5436/10000: Training Loss: 0.28363236137058423		 Val Loss: 1.7364860202955164
             	Training MAE: 0.0676572173833847		 Val MAE: 0.05504735931754112
             	Algebraic dist: 0.16622953829558		 Val Algebraic dist: 0.3190270299496858
             	RE1 dist: 0.030025552148404328		 Val RE1 dist: 0.21591298476509427
             	SED dist: 0.5050400858340056		 Val SED dist: 3.4413518490998642

Epoch 5437/10000: Training Loss: 0.3596266456272291		 Val Loss: 1.5907337354577107
             	Training MAE: 0.06854160130023956		 Val MAE: 0.06231885030865669
             	Algebraic dist: 0.19547829420670218		 Val Algebraic dist: 0.32080028368079144
             	RE1 dist: 0.039437874503757644		 Val RE1 dist: 0.18848008694856064
             	SED dist: 0.6557088934856913		 Val SED dist: 3.1408207105553667

Epoch 5438/10000: Training Loss: 0.26596483976944635		 Val Loss: 1.4565434663192085
             	Training MAE: 0.06747546046972275		 Val MAE: 0.05725182965397835
             	Algebraic dist: 0.16518490210823392		 Val Algebraic dist: 0.3213386742965035
             	RE1 dist: 0.02838678722796233		 Val RE1 dist: 0.1876519659291143
             	SED dist: 0.4690638832423998		 Val SED dist: 2.878443842348845

Epoch 5439/10000: Training Loss: 0.7297860021176545		 Val Loss: 2.3359154410984204
             	Training MAE: 0.06985101103782654		 Val MAE: 0.05408351495862007
             	Algebraic dist: 0.2974424984144128		 Val Algebraic dist: 0.43222991279933765
             	RE1 dist: 0.08050794705100682		 Val RE1 dist: 0.280721726624862
             	SED dist: 1.3945536406143852		 Val SED dist: 4.638602878736413

Epoch 5440/10000: Training Loss: 0.7979841646940812		 Val Loss: 1.9161796569824219
             	Training MAE: 0.06728500127792358		 Val MAE: 0.056035589426755905
             	Algebraic dist: 0.2808812182882558		 Val Algebraic dist: 0.3531885976376741
             	RE1 dist: 0.08647408174431842		 Val RE1 dist: 0.2248356238655422
             	SED dist: 1.535846876061481		 Val SED dist: 3.7999002207880435

Epoch 5441/10000: Training Loss: 0.29998603074446967		 Val Loss: 1.6263630079186482
             	Training MAE: 0.06712238490581512		 Val MAE: 0.07455668598413467
             	Algebraic dist: 0.17584255467290463		 Val Algebraic dist: 0.30952546907507855
             	RE1 dist: 0.031653665977975594		 Val RE1 dist: 0.18163088093633237
             	SED dist: 0.5380406172379203		 Val SED dist: 3.1972570004670517

slurmstepd: error: *** JOB 1783461 ON cs-4090-02 CANCELLED AT 2024-12-07T18:25:20 ***
