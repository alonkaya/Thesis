/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
/home/alonkay/Thesis/FMatrixRegressor.py:326: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, part: head, get_old_path: False,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: plots/Affine/BS_32__lr_6e-05__train_size_9216__CLIP__alpha_10__conv__original_rotated/model.pth,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.015, norm_mean: tensor([0.4815, 0.4578, 0.4082], device='cuda:0'), norm_std: tensor([0.2686, 0.2613, 0.2758], device='cuda:0'), sched: None seed: 42, 


##### CONTINUE TRAINING #####


Epoch 17004/20000: Training Loss: 0.3054251432418823		 Val Loss: 1.034233638218471
             	Training MAE: 0.05646585673093796		 Val MAE: 0.054989252239465714
             	Algebraic dist: 0.6237798690795898		 Val Algebraic dist: 1.3531951904296875
             	RE1 dist: 0.4027660846710205		 Val RE1 dist: 1.654543195452009
             	SED dist: 0.5843257904052734		 Val SED dist: 2.043859073093959


Epoch 17005/20000: Training Loss: 0.2626957416534424		 Val Loss: 0.987750734601702
             	Training MAE: 0.057825151830911636		 Val MAE: 0.057272303849458694
             	Algebraic dist: 0.5668574333190918		 Val Algebraic dist: 1.2752017974853516
             	RE1 dist: 0.34401252269744875		 Val RE1 dist: 1.5573885781424386
             	SED dist: 0.4989173889160156		 Val SED dist: 1.9497176579066686


Epoch 17006/20000: Training Loss: 0.23510270118713378		 Val Loss: 1.1195784977504186
             	Training MAE: 0.05634991452097893		 Val MAE: 0.05352913960814476
             	Algebraic dist: 0.527950382232666		 Val Algebraic dist: 1.3232415063040597
             	RE1 dist: 0.29042065143585205		 Val RE1 dist: 1.6389785494123186
             	SED dist: 0.4443665504455566		 Val SED dist: 2.214994158063616


Epoch 17007/20000: Training Loss: 0.26037888526916503		 Val Loss: 0.9828793661934989
             	Training MAE: 0.055676572024822235		 Val MAE: 0.055316317826509476
             	Algebraic dist: 0.558464527130127		 Val Algebraic dist: 1.243978772844587
             	RE1 dist: 0.3463809251785278		 Val RE1 dist: 1.5520902361188615
             	SED dist: 0.49526562690734866		 Val SED dist: 1.9409339087350028


Epoch 17008/20000: Training Loss: 0.27794198989868163		 Val Loss: 0.8879117284502301
             	Training MAE: 0.05736462026834488		 Val MAE: 0.05483044311404228
             	Algebraic dist: 0.5878476142883301		 Val Algebraic dist: 1.1601014818464006
             	RE1 dist: 0.35949082374572755		 Val RE1 dist: 1.316596576145717
             	SED dist: 0.5295028209686279		 Val SED dist: 1.7511249269757951


Epoch 17009/20000: Training Loss: 0.3161196708679199		 Val Loss: 1.0753677913120814
             	Training MAE: 0.05625844746828079		 Val MAE: 0.054473862051963806
             	Algebraic dist: 0.626625394821167		 Val Algebraic dist: 1.2517261505126953
             	RE1 dist: 0.395980978012085		 Val RE1 dist: 1.5835545403616769
             	SED dist: 0.6064945220947265		 Val SED dist: 2.1263038090297153


