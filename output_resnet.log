/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/Thesis/FMatrixRegressor.py:326: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: microsoft/resnet-152, augmentation: True, random crop: True, part: head, get_old_path: False,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.1, norm_mean: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), norm_std: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), sched: None seed: 42, 


##### CONTINUE TRAINING #####


Epoch 4014/4000: Training Loss: 0.3307651070987477		 Val Loss: 0.5107645781143851
             	Training MAE: 0.06616748869419098		 Val MAE: 0.06348865479230881
             	Algebraic dist: 0.4549261822420008		 Val Algebraic dist: 0.6463667413462764
             	RE1 dist: 0.24004089131074793		 Val RE1 dist: 0.5199712670367697
             	SED dist: 0.6254983228795669		 Val SED dist: 0.9876611129097317


Epoch 4015/4000: Training Loss: 0.20470906706417308		 Val Loss: 0.31851399463155994
             	Training MAE: 0.06404877454042435		 Val MAE: 0.06460632383823395
             	Algebraic dist: 0.3859546324786018		 Val Algebraic dist: 0.4834138206813646
             	RE1 dist: 0.15520769007065716		 Val RE1 dist: 0.246331090512483
             	SED dist: 0.3748457011054544		 Val SED dist: 0.6029458667920984


Epoch 4016/4000: Training Loss: 0.22520105979021857		 Val Loss: 0.39164418759553327
             	Training MAE: 0.06464673578739166		 Val MAE: 0.06365428864955902
             	Algebraic dist: 0.4125659606036018		 Val Algebraic dist: 0.5536145334658416
             	RE1 dist: 0.1647973200854133		 Val RE1 dist: 0.2993506555971892
             	SED dist: 0.41545890359317555		 Val SED dist: 0.7494428883428159


Epoch 4017/4000: Training Loss: 0.20996682784136603		 Val Loss: 0.3498372616975204
             	Training MAE: 0.0643705278635025		 Val MAE: 0.06633234024047852
             	Algebraic dist: 0.38883405573227825		 Val Algebraic dist: 0.48160523953645124
             	RE1 dist: 0.1567808039048139		 Val RE1 dist: 0.26540291827657947
             	SED dist: 0.38518501730526195		 Val SED dist: 0.6648601034413213


Epoch 4018/4000: Training Loss: 0.7994135127348059		 Val Loss: 0.6553731171981149
             	Training MAE: 0.0682004764676094		 Val MAE: 0.06635711342096329
             	Algebraic dist: 0.8167023939244887		 Val Algebraic dist: 0.8480388807213824
             	RE1 dist: 0.7407767912920784		 Val RE1 dist: 0.6878905503646188
             	SED dist: 1.5615030176499312		 Val SED dist: 1.2763977880063264


Epoch 4019/4000: Training Loss: 0.23769819035249598		 Val Loss: 1.0078698863153872
             	Training MAE: 0.0643450990319252		 Val MAE: 0.06584743410348892
             	Algebraic dist: 0.45189781749949737		 Val Algebraic dist: 1.0443213089652683
             	RE1 dist: 0.21065977040459127		 Val RE1 dist: 1.160944648410963
             	SED dist: 0.44116368013269763		 Val SED dist: 1.9813738283903704


Epoch 4020/4000: Training Loss: 0.2713618558995864		 Val Loss: 0.6395051375679348
             	Training MAE: 0.06474314630031586		 Val MAE: 0.0621359646320343
             	Algebraic dist: 0.46185072730569277		 Val Algebraic dist: 0.8240230394446332
             	RE1 dist: 0.23623096241670496		 Val RE1 dist: 0.6125707626342773
             	SED dist: 0.5082162969252643		 Val SED dist: 1.2458472044571587


Epoch 4021/4000: Training Loss: 0.1838355905869428		 Val Loss: 0.3610469983971637
             	Training MAE: 0.0641060471534729		 Val MAE: 0.06388041377067566
             	Algebraic dist: 0.3739770720986759		 Val Algebraic dist: 0.519223461980405
             	RE1 dist: 0.1396311170914594		 Val RE1 dist: 0.30175157215284265
             	SED dist: 0.33314909654505115		 Val SED dist: 0.6882452757462211


Epoch 4022/4000: Training Loss: 0.26594243330114026		 Val Loss: 0.46480556156324304
             	Training MAE: 0.06451105326414108		 Val MAE: 0.06134079396724701
             	Algebraic dist: 0.4368895362405216		 Val Algebraic dist: 0.6128637065058169
             	RE1 dist: 0.2026202398187974		 Val RE1 dist: 0.47047627490499744
             	SED dist: 0.497041758368997		 Val SED dist: 0.8963072403617527


Epoch 4023/4000: Training Loss: 0.23987422269933364		 Val Loss: 0.5668185275533925
             	Training MAE: 0.06597253680229187		 Val MAE: 0.06022155284881592
             	Algebraic dist: 0.4329138924093807		 Val Algebraic dist: 0.7414316923721976
             	RE1 dist: 0.1844200246474322		 Val RE1 dist: 0.5249829499617867
             	SED dist: 0.44416189193725586		 Val SED dist: 1.1004617939824644


Epoch 4024/4000: Training Loss: 0.193907990175135		 Val Loss: 0.3447660777879798
             	Training MAE: 0.06450047343969345		 Val MAE: 0.06186280772089958
             	Algebraic dist: 0.3787232286789838		 Val Algebraic dist: 0.49232188515041186
             	RE1 dist: 0.14577353701871984		 Val RE1 dist: 0.2601403153460959
             	SED dist: 0.35305040022906137		 Val SED dist: 0.6561620131782864


Epoch 4025/4000: Training Loss: 0.32695865631103516		 Val Loss: 0.5535359589949899
             	Training MAE: 0.06569263339042664		 Val MAE: 0.06105891987681389
             	Algebraic dist: 0.5122906179989085		 Val Algebraic dist: 0.7384014129638672
             	RE1 dist: 0.2643827270059025		 Val RE1 dist: 0.5030916048132855
             	SED dist: 0.6185290392707375		 Val SED dist: 1.0739639116370159


Epoch 4026/4000: Training Loss: 0.17333399548250086		 Val Loss: 0.34246815805849823
             	Training MAE: 0.06410222500562668		 Val MAE: 0.06402818113565445
             	Algebraic dist: 0.3538113762350643		 Val Algebraic dist: 0.48107520393703296
             	RE1 dist: 0.1289839604321648		 Val RE1 dist: 0.2893342971801758
             	SED dist: 0.3121161180384019		 Val SED dist: 0.6509549099466075


Epoch 4027/4000: Training Loss: 0.15483682295855353		 Val Loss: 0.588684952777365
             	Training MAE: 0.0642424076795578		 Val MAE: 0.06206626817584038
             	Algebraic dist: 0.3219624687643612		 Val Algebraic dist: 0.7590361885402513
             	RE1 dist: 0.10810152923359591		 Val RE1 dist: 0.4498764121014139
             	SED dist: 0.2748447025523466		 Val SED dist: 1.144136843474015


Epoch 4028/4000: Training Loss: 0.2971798391903148		 Val Loss: 0.37182210839313007
             	Training MAE: 0.06496952474117279		 Val MAE: 0.061655450612306595
             	Algebraic dist: 0.46896735359640684		 Val Algebraic dist: 0.5059897795967434
             	RE1 dist: 0.22575189085567698		 Val RE1 dist: 0.3359295596247134
             	SED dist: 0.5592604244456572		 Val SED dist: 0.710318606832753


Epoch 4029/4000: Training Loss: 0.2410580971661736		 Val Loss: 0.7674049709154211
             	Training MAE: 0.06458369642496109		 Val MAE: 0.06353341788053513
             	Algebraic dist: 0.4148900929619284		 Val Algebraic dist: 0.8929653997006624
             	RE1 dist: 0.17880599638995		 Val RE1 dist: 0.6478693588920261
             	SED dist: 0.4471443400663488		 Val SED dist: 1.5009984555451765


Epoch 4030/4000: Training Loss: 0.3456598169663373		 Val Loss: 0.4952345723691194
             	Training MAE: 0.06468455493450165		 Val MAE: 0.060550615191459656
             	Algebraic dist: 0.5437576069551355		 Val Algebraic dist: 0.6279773712158203
             	RE1 dist: 0.291925822987276		 Val RE1 dist: 0.44767160001008405
             	SED dist: 0.6566538530237535		 Val SED dist: 0.957603703374448


Epoch 4031/4000: Training Loss: 0.19869612244998708		 Val Loss: 0.37313150322955585
             	Training MAE: 0.064633309841156		 Val MAE: 0.06923884898424149
             	Algebraic dist: 0.38558300803689394		 Val Algebraic dist: 0.5083159571108611
             	RE1 dist: 0.15417197171379537		 Val RE1 dist: 0.31322935353154724
             	SED dist: 0.3627351312076344		 Val SED dist: 0.7098888728929602


