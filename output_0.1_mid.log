nohup: ignoring input
/home/alonkay/conda/alon/lib/python3.9/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 805: MPS client failed to connect to the MPS control daemon or the MPS server (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)
  return torch._C._cuda_getDeviceCount() > 0
/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
###########################################################################################################################################################

                         learning rate: 0.0001, lr_decay: 0.8, mlp_hidden_sizes: [1024, 512], jump_frames: 2, use_reconstruction_layer: True
                        batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], dataset: Stereo,
                        average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, deepF_nocorrs: False, part: mid, get_old_path: False,
                        SVD coeff: 0, RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 4, trained vit: None,
                        crop: 224 resize: 256, use conv: False pretrained: None, data_ratio: 0.1, norm_mean: tensor([0.4815, 0.4578, 0.4082]), norm_std: tensor([0.2686, 0.2613, 0.2758]), sched: None seed: 200, 


algebraic_truth: 0.0167390458724078		 val_algebraic_truth: 0.017267291960508926
RE1_truth: 0.00018920101608861894		 val_RE1_truth: 0.00020104535328953162
SED_truth: 0.00151348059229991		 val_SED_truth: 0.0016083680093288422


Epoch 1/4001: Training Loss: 3726.8005514705883		 Val Loss: 2654.7550951086955
             Training MAE: 0.3285462558269501		 Val MAE: 0.3212648332118988
             Algebraic dist: 706.9416360294117		 Val Algebraic dist: 24.606668223505434
             RE1 dist: 3704326.588235294		 Val RE1 dist: 912.4487092391304
             SED dist: 7452.659926470588		 Val SED dist: 5308.555027173913


Epoch 2/4001: Training Loss: 930.1021943933823		 Val Loss: 6.109802909519361
             Training MAE: 0.33462029695510864		 Val MAE: 0.32183051109313965
             Algebraic dist: 47.497138528262866		 Val Algebraic dist: 70.37839143172555
             RE1 dist: 6798.927389705882		 Val RE1 dist: 6899.587635869565
             SED dist: 1859.2859604779412		 Val SED dist: 11.687251878821332


Epoch 3/4001: Training Loss: 2.579393947825712		 Val Loss: 5.687496019446331
             Training MAE: 0.27757957577705383		 Val MAE: 0.2339838296175003
             Algebraic dist: 57.675271426930145		 Val Algebraic dist: 62.88460109544837
             RE1 dist: 4038.278033088235		 Val RE1 dist: 5404.071671195652
             SED dist: 4.743442759794347		 Val SED dist: 11.096152264138926


Epoch 4/4001: Training Loss: 2.7588121750775505		 Val Loss: 4.9318064814028535
             Training MAE: 0.2556939125061035		 Val MAE: 0.22917965054512024
             Algebraic dist: 69.10944680606617		 Val Algebraic dist: 63.26355511209239
             RE1 dist: 5689.344209558823		 Val RE1 dist: 4857.496942934783
             SED dist: 5.1815284280216		 Val SED dist: 9.60077435037364


Epoch 5/4001: Training Loss: 2.2245429543887867		 Val Loss: 4.436531398607337
             Training MAE: 0.23800908029079437		 Val MAE: 0.23535549640655518
             Algebraic dist: 57.828606100643384		 Val Algebraic dist: 76.95036514945652
             RE1 dist: 4026.7826286764707		 Val RE1 dist: 7317.249320652174
             SED dist: 4.1618203555836395		 Val SED dist: 8.600232331649117


Epoch 6/4001: Training Loss: 2.498935250674977		 Val Loss: 5.07172095257303
             Training MAE: 0.23571692407131195		 Val MAE: 0.22035038471221924
             Algebraic dist: 66.22206744025735		 Val Algebraic dist: 79.6082179857337
             RE1 dist: 4926.097426470588		 Val RE1 dist: 7938.426630434783
             SED dist: 4.720100402832031		 Val SED dist: 9.90094524881114


Epoch 7/4001: Training Loss: 2.3930509230669808		 Val Loss: 4.664212102475374
             Training MAE: 0.2339693307876587		 Val MAE: 0.2376832365989685
             Algebraic dist: 63.78787051930147		 Val Algebraic dist: 79.1801173997962
             RE1 dist: 4706.670955882353		 Val RE1 dist: 7080.32472826087
             SED dist: 4.513529160443475		 Val SED dist: 9.057507058848506


Epoch 8/4001: Training Loss: 2.529852025649127		 Val Loss: 5.23213063115659
             Training MAE: 0.2365047037601471		 Val MAE: 0.22746124863624573
             Algebraic dist: 68.68777286305146		 Val Algebraic dist: 85.74991508152173
             RE1 dist: 5330.366268382353		 Val RE1 dist: 10050.528532608696
             SED dist: 4.78048391903148		 Val SED dist: 10.21000273331352


Epoch 9/4001: Training Loss: 2.04479419483858		 Val Loss: 4.765018960703975
             Training MAE: 0.2229536920785904		 Val MAE: 0.25661447644233704
             Algebraic dist: 58.66270536534927		 Val Algebraic dist: 111.87452233355978
             RE1 dist: 4145.2109375		 Val RE1 dist: 12985.767663043478
             SED dist: 3.839445675120634		 Val SED dist: 9.202582317849863


Epoch 10/4001: Training Loss: 2.1019280377556298		 Val Loss: 5.716331813646399
             Training MAE: 0.2209821343421936		 Val MAE: 0.22238469123840332
             Algebraic dist: 59.39338594324448		 Val Algebraic dist: 85.4832179857337
             RE1 dist: 4012.5445772058824		 Val RE1 dist: 10925.20652173913
             SED dist: 3.9598859899184284		 Val SED dist: 11.191442074983016


Epoch 11/4001: Training Loss: 2.230304718017578		 Val Loss: 13.19375875721807
             Training MAE: 0.22550368309020996		 Val MAE: 0.23759794235229492
             Algebraic dist: 63.81390739889706		 Val Algebraic dist: 145.45801842730978
             RE1 dist: 4581.56387867647		 Val RE1 dist: 25181.26358695652
             SED dist: 4.2106789981617645		 Val SED dist: 26.11968463400136


Epoch 12/4001: Training Loss: 2.8821927238913143		 Val Loss: 5.268222974694294
             Training MAE: 0.23257631063461304		 Val MAE: 0.233899787068367
             Algebraic dist: 74.00045955882354		 Val Algebraic dist: 99.41251273777173
             RE1 dist: 6365.909466911765		 Val RE1 dist: 14679.551630434782
             SED dist: 5.498949836282169		 Val SED dist: 10.26599651834239


Epoch 13/4001: Training Loss: 1.92279613719267		 Val Loss: 6.340519117272419
             Training MAE: 0.21757124364376068		 Val MAE: 0.20603153109550476
             Algebraic dist: 57.02603687959559		 Val Algebraic dist: 85.22181768002717
             RE1 dist: 3902.737591911765		 Val RE1 dist: 10982.646739130434
             SED dist: 3.607685313505285		 Val SED dist: 12.466405453889266


Epoch 14/4001: Training Loss: 2.0350429310518154		 Val Loss: 4.706880652386209
             Training MAE: 0.21296606957912445		 Val MAE: 0.22612689435482025
             Algebraic dist: 56.96765495749081		 Val Algebraic dist: 85.31151282269022
             RE1 dist: 3695.341681985294		 Val RE1 dist: 8513.644701086956
             SED dist: 3.8426114250631893		 Val SED dist: 9.169145003609035


Epoch 15/4001: Training Loss: 2.1045866573558136		 Val Loss: 5.617066093113111
             Training MAE: 0.21236075460910797		 Val MAE: 0.20669913291931152
             Algebraic dist: 58.70481962316177		 Val Algebraic dist: 81.64308232846467
             RE1 dist: 3920.541819852941		 Val RE1 dist: 9214.315217391304
             SED dist: 3.983685661764706		 Val SED dist: 11.02225461213485


Epoch 16/4001: Training Loss: 2.2354888916015625		 Val Loss: 4.847203462020211
             Training MAE: 0.21299175918102264		 Val MAE: 0.22027750313282013
             Algebraic dist: 61.75264964384191		 Val Algebraic dist: 86.11143958050272
             RE1 dist: 4367.441176470588		 Val RE1 dist: 10019.819972826086
             SED dist: 4.242649302763097		 Val SED dist: 9.459357220193613


Epoch 17/4001: Training Loss: 1.9525570588953354		 Val Loss: 4.478553440259851
             Training MAE: 0.20567865669727325		 Val MAE: 0.2197341024875641
             Algebraic dist: 55.698005227481616		 Val Algebraic dist: 81.73641835088316
             RE1 dist: 3543.542279411765		 Val RE1 dist: 8467.932065217392
             SED dist: 3.691752489875345		 Val SED dist: 8.723447053328805


68


30007536515


