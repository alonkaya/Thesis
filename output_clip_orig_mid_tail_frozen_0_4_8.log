nohup: ignoring input
/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
/home/aviran/Alon/Thesis/FMatrixRegressor.py:339: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.004__mid__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.004__mid__frozen_4
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.004__mid__frozen_8
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.004__tail__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.004__tail__frozen_4
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.004__tail__frozen_8
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.008__mid__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.008__mid__frozen_4
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.008__mid__frozen_8
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.008__tail__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.008__tail__frozen_4
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.008__tail__frozen_8
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.1__mid__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.1__mid__frozen_4
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.1__mid__frozen_8
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.1__tail__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.1__tail__frozen_4
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.1__tail__frozen_8
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/BS_8__ratio_0.2__mid__frozen_0

###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, part: mid, get_old_path: False, computer: 0,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 4, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.2, norm_mean: tensor([0.4815, 0.4578, 0.4082], device='cuda:0'), norm_std: tensor([0.2686, 0.2613, 0.2758], device='cuda:0'), sched: None seed: 42
UPDATED STOP! 

train size: 2170, val size: 738, test size: 1064

##### CONTINUE TRAINING #####


################
Empty points at 03


Epoch 3118/4500: Training Loss: 0.22054339857662425		 Val Loss: 0.2835552256594422
             	Training MAE: 0.06282154470682144		 Val MAE: 0.058275215327739716
             	Algebraic dist: 0.33623560737161073		 Val Algebraic dist: 0.3365101557905956
             	RE1 dist: 0.11102003209731158		 Val RE1 dist: 0.15123036087200206
             	SED dist: 0.40771181443158316		 Val SED dist: 0.5355110988822035


################
Empty points at 03


Epoch 3119/4500: Training Loss: 0.18956180179820342		 Val Loss: 0.47173694897723456
             	Training MAE: 0.061724502593278885		 Val MAE: 0.06214376166462898
             	Algebraic dist: 0.30880386689129996		 Val Algebraic dist: 0.4375910194971228
             	RE1 dist: 0.09300208792966955		 Val RE1 dist: 0.2930831704088437
             	SED dist: 0.3462434936972225		 Val SED dist: 0.9107016286542339

Epoch 3120/4500: Training Loss: 0.17534167626324823		 Val Loss: 0.6568503841277091
             	Training MAE: 0.061624545603990555		 Val MAE: 0.05970385670661926
             	Algebraic dist: 0.29170361687155333		 Val Algebraic dist: 0.544913589313466
             	RE1 dist: 0.08534911099602194		 Val RE1 dist: 0.3967794602917087
             	SED dist: 0.31777168722713695		 Val SED dist: 1.2816403296685988


################
Empty points at 03


Epoch 3121/4500: Training Loss: 0.1918196818407844		 Val Loss: 0.33543541610881844
             	Training MAE: 0.06190557777881622		 Val MAE: 0.05781170353293419
             	Algebraic dist: 0.3114635523627786		 Val Algebraic dist: 0.3779497454243322
             	RE1 dist: 0.09417869764215805		 Val RE1 dist: 0.18696307110530075
             	SED dist: 0.35062287835513845		 Val SED dist: 0.6394775144515499


################
Empty points at 03


Epoch 3122/4500: Training Loss: 0.20529350112466252		 Val Loss: 0.2597590620799731
             	Training MAE: 0.06290993094444275		 Val MAE: 0.061556316912174225
             	Algebraic dist: 0.3265705108642578		 Val Algebraic dist: 0.32435474600843206
             	RE1 dist: 0.10284182604621439		 Val RE1 dist: 0.14278562607303744
             	SED dist: 0.3772512884701		 Val SED dist: 0.4868735856907342


################
Empty points at 03


Epoch 3123/4500: Training Loss: 0.22270317638621612		 Val Loss: 0.2584043728407993
             	Training MAE: 0.061743564903736115		 Val MAE: 0.06030412018299103
             	Algebraic dist: 0.33274053124820485		 Val Algebraic dist: 0.3366443675051453
             	RE1 dist: 0.11456016933216769		 Val RE1 dist: 0.1389468510945638
             	SED dist: 0.4125554982353659		 Val SED dist: 0.48467283351446994

Epoch 3124/4500: Training Loss: 0.16613078117370605		 Val Loss: 0.297323247437836
             	Training MAE: 0.06172412633895874		 Val MAE: 0.05855299159884453
             	Algebraic dist: 0.2902849141289206		 Val Algebraic dist: 0.3892501913091188
             	RE1 dist: 0.08021120464100558		 Val RE1 dist: 0.1491470747096564
             	SED dist: 0.2993922794566435		 Val SED dist: 0.5628439380276588

Epoch 3125/4500: Training Loss: 0.30266660802504597		 Val Loss: 0.3225353199948547
             	Training MAE: 0.06289767473936081		 Val MAE: 0.057953495532274246
             	Algebraic dist: 0.3916009734658634		 Val Algebraic dist: 0.39038758636802756
             	RE1 dist: 0.1584659043480368		 Val RE1 dist: 0.1836447561940839
             	SED dist: 0.5718347886029411		 Val SED dist: 0.613780031922043

Epoch 3126/4500: Training Loss: 0.15079143468071432		 Val Loss: 0.35726699008736557
             	Training MAE: 0.06155397742986679		 Val MAE: 0.05788165330886841
             	Algebraic dist: 0.27233095730052276		 Val Algebraic dist: 0.45161208286080307
             	RE1 dist: 0.07369023912093219		 Val RE1 dist: 0.18653927054456484
             	SED dist: 0.2688297944910386		 Val SED dist: 0.6830722029491137

Epoch 3127/4500: Training Loss: 0.2559358652900247		 Val Loss: 0.610797594952327
             	Training MAE: 0.06187732517719269		 Val MAE: 0.05835777521133423
             	Algebraic dist: 0.35927685569314394		 Val Algebraic dist: 0.574289301390289
             	RE1 dist: 0.13326354587779327		 Val RE1 dist: 0.38115113781344506
             	SED dist: 0.47895313711727366		 Val SED dist: 1.189678725375924

Epoch 3128/4500: Training Loss: 0.19950260835535386		 Val Loss: 0.39699414981308806
             	Training MAE: 0.06174936518073082		 Val MAE: 0.0633084774017334
             	Algebraic dist: 0.31754061755012064		 Val Algebraic dist: 0.49282488258936075
             	RE1 dist: 0.10067127031438491		 Val RE1 dist: 0.20557469193653394
             	SED dist: 0.3659910314223346		 Val SED dist: 0.7611157817225302

Epoch 3129/4500: Training Loss: 0.204057118471931		 Val Loss: 0.24400934609033728
             	Training MAE: 0.06202393397688866		 Val MAE: 0.058561887592077255
             	Algebraic dist: 0.31637146893669577		 Val Algebraic dist: 0.3261503404186618
             	RE1 dist: 0.1048600252936868		 Val RE1 dist: 0.1315737078266759
             	SED dist: 0.3749881912680233		 Val SED dist: 0.4561256900910408

Epoch 3130/4500: Training Loss: 0.18249959104201374		 Val Loss: 0.4050585428873698
             	Training MAE: 0.06187448650598526		 Val MAE: 0.05850832536816597
             	Algebraic dist: 0.29210250517901254		 Val Algebraic dist: 0.42701868857106856
             	RE1 dist: 0.08990800380706787		 Val RE1 dist: 0.24165301169118575
             	SED dist: 0.3320143082562615		 Val SED dist: 0.7780035695722026


################
Empty points at 03


Epoch 3131/4500: Training Loss: 0.17360371701857624		 Val Loss: 0.3512777513073337
             	Training MAE: 0.06078032776713371		 Val MAE: 0.05803222581744194
             	Algebraic dist: 0.2933515941395479		 Val Algebraic dist: 0.43719617782100556
             	RE1 dist: 0.08459833790274228		 Val RE1 dist: 0.18595580644504997
             	SED dist: 0.31471743303186756		 Val SED dist: 0.6709252634356099

Epoch 3132/4500: Training Loss: 0.20558075343861298		 Val Loss: 0.8681947441511256
             	Training MAE: 0.06315938383340836		 Val MAE: 0.05836367607116699
             	Algebraic dist: 0.3221730063943302		 Val Algebraic dist: 0.7694622573032174
             	RE1 dist: 0.10258196381961598		 Val RE1 dist: 0.4680192598732569
             	SED dist: 0.3775478250840131		 Val SED dist: 1.7045824604649698


################
Empty points at 03


Epoch 3133/4500: Training Loss: 0.23787209566901713		 Val Loss: 0.3201842769499748
             	Training MAE: 0.061693623661994934		 Val MAE: 0.0667947456240654
             	Algebraic dist: 0.34301283780266256		 Val Algebraic dist: 0.3873179856167045
             	RE1 dist: 0.12296556023990407		 Val RE1 dist: 0.17324115384009578
             	SED dist: 0.4429631233215332		 Val SED dist: 0.6055941838090138


################
Empty points at 03


Epoch 3134/4500: Training Loss: 0.1351926186505486		 Val Loss: 0.34209003243395075
             	Training MAE: 0.06162251532077789		 Val MAE: 0.06038915365934372
             	Algebraic dist: 0.24936639561372645		 Val Algebraic dist: 0.3656891853578629
             	RE1 dist: 0.06244670643525965		 Val RE1 dist: 0.19318572423791372
             	SED dist: 0.23756560157327092		 Val SED dist: 0.6519249126475345

Epoch 3135/4500: Training Loss: 0.17363511814790614		 Val Loss: 0.24407618532898606
             	Training MAE: 0.062440574169158936		 Val MAE: 0.06452641636133194
             	Algebraic dist: 0.286708018359016		 Val Algebraic dist: 0.3204518595049458
             	RE1 dist: 0.08371661691104665		 Val RE1 dist: 0.12861453845936766
             	SED dist: 0.31404944027171416		 Val SED dist: 0.4545650687268985

Epoch 3136/4500: Training Loss: 0.23593218186322382		 Val Loss: 0.24961748430805822
             	Training MAE: 0.06346013396978378		 Val MAE: 0.06568369269371033
             	Algebraic dist: 0.34817900377161365		 Val Algebraic dist: 0.32232594233687206
             	RE1 dist: 0.12010495802935432		 Val RE1 dist: 0.13272139846637684
             	SED dist: 0.4380543933195226		 Val SED dist: 0.4652615618962114


################
Empty points at 03


Epoch 3137/4500: Training Loss: 0.178682102876551		 Val Loss: 0.3931824878979755
             	Training MAE: 0.061788856983184814		 Val MAE: 0.058400318026542664
             	Algebraic dist: 0.29798493665807385		 Val Algebraic dist: 0.3933258877005628
             	RE1 dist: 0.08633848498849307		 Val RE1 dist: 0.23663635664088753
             	SED dist: 0.32452740388758045		 Val SED dist: 0.754445311843708

Epoch 3138/4500: Training Loss: 0.1625670825733858		 Val Loss: 0.2651827412266885
             	Training MAE: 0.06195683032274246		 Val MAE: 0.06067994236946106
             	Algebraic dist: 0.28112613453584556		 Val Algebraic dist: 0.3285824765441238
             	RE1 dist: 0.07782887711244471		 Val RE1 dist: 0.15308553429060084
             	SED dist: 0.292202725129969		 Val SED dist: 0.497787352531187


################
Empty points at 03


Epoch 3139/4500: Training Loss: 0.23995699602014878		 Val Loss: 1.0719525737147177
             	Training MAE: 0.06138179823756218		 Val MAE: 0.05873250588774681
             	Algebraic dist: 0.3510392974404728		 Val Algebraic dist: 0.8490293769426244
             	RE1 dist: 0.1218561004189884		 Val RE1 dist: 0.5716409170499412
             	SED dist: 0.44719429577098174		 Val SED dist: 2.111605408371136


################
Empty points at 03


Epoch 3140/4500: Training Loss: 0.28382550968843345		 Val Loss: 0.33904254051946825
             	Training MAE: 0.061866093426942825		 Val MAE: 0.06269678473472595
             	Algebraic dist: 0.3783709021175609		 Val Algebraic dist: 0.36833478045719925
             	RE1 dist: 0.1526049866395838		 Val RE1 dist: 0.20078667261267222
             	SED dist: 0.5347471798167509		 Val SED dist: 0.6455042439122354

Epoch 3141/4500: Training Loss: 0.17336479355307186		 Val Loss: 0.537406265094716
             	Training MAE: 0.06114404648542404		 Val MAE: 0.058385565876960754
             	Algebraic dist: 0.29829103806439566		 Val Algebraic dist: 0.6033485986853159
             	RE1 dist: 0.08694656456218046		 Val RE1 dist: 0.28673604739609587
             	SED dist: 0.3141886486726649		 Val SED dist: 1.042977774015037

Fatal Python error: Illegal instruction

Thread 0x00007ee425e00640 (most recent call first):
<no Python frame>

Current thread 0x00007ee5aeef3740 (most recent call first):
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torchvision/transforms/v2/functional/_misc.py", line 45 in normalize_image
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py", line 35 in _call_kernel
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torchvision/transforms/v2/_misc.py", line 165 in _transform
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py", line 51 in <listcomp>
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torchvision/transforms/v2/_transform.py", line 50 in forward
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562 in _call_impl
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553 in _wrapped_call_impl
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torchvision/transforms/v2/_container.py", line 51 in forward
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562 in _call_impl
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553 in _wrapped_call_impl
  File "/home/aviran/Alon/Thesis/Dataset.py", line 96 in __getitem__
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/utils/data/dataset.py", line 350 in __getitem__
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52 in <listcomp>
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 52 in fetch
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 673 in _next_data
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 630 in __next__
  File "/home/aviran/Alon/Thesis/FMatrixRegressor.py", line 263 in dataloader_step
  File "/home/aviran/Alon/Thesis/FMatrixRegressor.py", line 190 in train_model
  File "/home/aviran/Alon/Thesis/Main.py", line 115 in <module>
