/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/Thesis/FMatrixRegressor.py:333: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.2__head__frozen_0

###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: microsoft/resnet-152, augmentation: True, random crop: True, part: head, get_old_path: False, computer: 1,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.1, norm_mean: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), norm_std: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), sched: None seed: 42, 

train size: 1082, val size: 367, test size: 1064

##### CONTINUE TRAINING #####

Epoch 4761/7000: Training Loss: 0.2904196626999799		 Val Loss: 0.6827108134394106
             	Training MAE: 0.06383898109197617		 Val MAE: 0.06836853176355362
             	Algebraic dist: 0.49146517585305605		 Val Algebraic dist: 0.7465293718420941
             	RE1 dist: 0.2541930815752815		 Val RE1 dist: 0.7467215164847996
             	SED dist: 0.5468211454503676		 Val SED dist: 1.3297343876050867

Epoch 4762/7000: Training Loss: 0.31096363067626953		 Val Loss: 0.6221332135407821
             	Training MAE: 0.06509336829185486		 Val MAE: 0.06134035065770149
             	Algebraic dist: 0.5120348088881549		 Val Algebraic dist: 0.8070473048997961
             	RE1 dist: 0.26639211879057045		 Val RE1 dist: 0.5855539985325026
             	SED dist: 0.5870755139519187		 Val SED dist: 1.21117815764054

Epoch 4763/7000: Training Loss: 0.1972191193524529		 Val Loss: 0.776138969089674
             	Training MAE: 0.06449780613183975		 Val MAE: 0.06029907986521721
             	Algebraic dist: 0.3756563242743997		 Val Algebraic dist: 0.912687467492145
             	RE1 dist: 0.14762512375326717		 Val RE1 dist: 0.7159357485563859
             	SED dist: 0.35976962482228		 Val SED dist: 1.5194692197053328

### Not decreasing ###

Epoch 4764/7000: Training Loss: 0.200274621739107		 Val Loss: 0.7722449924634851
             	Training MAE: 0.06466446816921234		 Val MAE: 0.06328919529914856
             	Algebraic dist: 0.37767449547262755		 Val Algebraic dist: 0.8655331653097401
             	RE1 dist: 0.14371327792896943		 Val RE1 dist: 0.6919934231302013
             	SED dist: 0.36569657045252185		 Val SED dist: 1.5101762854534646

### Not decreasing ###

Epoch 4765/7000: Training Loss: 0.17492140040678136		 Val Loss: 0.3045535087585449
             	Training MAE: 0.0644797533750534		 Val MAE: 0.06228110194206238
             	Algebraic dist: 0.34377381380866556		 Val Algebraic dist: 0.45709211929984717
             	RE1 dist: 0.11974280020769905		 Val RE1 dist: 0.23675188810929007
             	SED dist: 0.31488631753360524		 Val SED dist: 0.5751916636591372

### Not decreasing ###

Epoch 4766/7000: Training Loss: 0.21222915368921616		 Val Loss: 0.40794853542162024
             	Training MAE: 0.06427332758903503		 Val MAE: 0.06191223859786987
             	Algebraic dist: 0.38369397556080537		 Val Algebraic dist: 0.5579138631406038
             	RE1 dist: 0.15270038212046905		 Val RE1 dist: 0.298679372538691
             	SED dist: 0.3895880474763758		 Val SED dist: 0.7821013409158458

### Not decreasing ###

Epoch 4767/7000: Training Loss: 0.21504940706140854		 Val Loss: 0.38386842478876526
             	Training MAE: 0.06569407135248184		 Val MAE: 0.06801284104585648
             	Algebraic dist: 0.38529154833625345		 Val Algebraic dist: 0.4988786863244098
             	RE1 dist: 0.14760990703807159		 Val RE1 dist: 0.282573471898618
             	SED dist: 0.3944510852589327		 Val SED dist: 0.731988409291143

### Not decreasing ###

Epoch 4768/7000: Training Loss: 0.2142772113575655		 Val Loss: 0.5408159753550654
             	Training MAE: 0.0655445009469986		 Val MAE: 0.06316964328289032
             	Algebraic dist: 0.39383408602546244		 Val Algebraic dist: 0.6131217790686566
             	RE1 dist: 0.14750505896175609		 Val RE1 dist: 0.37950830874235736
             	SED dist: 0.393033223993638		 Val SED dist: 1.0474181797193445

### Not decreasing ###

Epoch 4769/7000: Training Loss: 0.2579820296343635		 Val Loss: 0.6207792862601902
             	Training MAE: 0.0643162876367569		 Val MAE: 0.06039777770638466
             	Algebraic dist: 0.4266771989710191		 Val Algebraic dist: 0.6041298327238663
             	RE1 dist: 0.18735463478985956		 Val RE1 dist: 0.45244760098664655
             	SED dist: 0.48114636365105123		 Val SED dist: 1.20801651996115

### Not decreasing ###

Epoch 4770/7000: Training Loss: 0.19582463713253245		 Val Loss: 0.39802314924157184
             	Training MAE: 0.06368875503540039		 Val MAE: 0.062107738107442856
             	Algebraic dist: 0.36893165812772866		 Val Algebraic dist: 0.5644309002420177
             	RE1 dist: 0.1373746254864861		 Val RE1 dist: 0.30773038449494733
             	SED dist: 0.3571598389569451		 Val SED dist: 0.7623955270518428

### Not decreasing ###

Epoch 4771/7000: Training Loss: 0.18627492119284236		 Val Loss: 0.32253269527269446
             	Training MAE: 0.06435224413871765		 Val MAE: 0.05998437479138374
             	Algebraic dist: 0.35204163719626036		 Val Algebraic dist: 0.4582454017970873
             	RE1 dist: 0.12649422533371868		 Val RE1 dist: 0.2443937633348548
             	SED dist: 0.3376050275914809		 Val SED dist: 0.6117778861004374

### Not decreasing ###

Epoch 4772/7000: Training Loss: 0.17217558972975788		 Val Loss: 0.5452398631883704
             	Training MAE: 0.06454391032457352		 Val MAE: 0.060245756059885025
             	Algebraic dist: 0.337431458865895		 Val Algebraic dist: 0.6967205379320227
             	RE1 dist: 0.11304520158206716		 Val RE1 dist: 0.46424376446267834
             	SED dist: 0.309232936185949		 Val SED dist: 1.0570102359937585

### Not decreasing ###

Epoch 4773/7000: Training Loss: 0.1722745194154627		 Val Loss: 0.2755964113318402
             	Training MAE: 0.06467819213867188		 Val MAE: 0.059732843190431595
             	Algebraic dist: 0.33753571790807385		 Val Algebraic dist: 0.4356959384420644
             	RE1 dist: 0.11170050677131205		 Val RE1 dist: 0.20156990963479746
             	SED dist: 0.3094169392305262		 Val SED dist: 0.5180330276489258

### Not decreasing ###

Epoch 1/10: Test SED dist: 0.48763728321046756
Epoch 2/10: Test SED dist: 0.4969380314188792
Epoch 3/10: Test SED dist: 0.4796905804397468
Epoch 4/10: Test SED dist: 0.507467972604852
Epoch 5/10: Test SED dist: 0.4874330678380522
Epoch 6/10: Test SED dist: 0.5096051782593691
Epoch 7/10: Test SED dist: 0.4952637521844161
Epoch 8/10: Test SED dist: 0.4933684499640214
Epoch 9/10: Test SED dist: 0.48790362365263745
Epoch 10/10: Test SED dist: 0.4908641729139744


## TEST RESULTS: ##
Test Loss: 0.2633878033860285		 Test MAE: 0.05989096835255623
Test Algebraic dist: 0.39506020510107054
Test SED dist: 0.49361721124864155
Test RE1 dist: 0.2043783101820408

Test Algebraic dist truth: 0.01766761801296607
Test SED dist truth: 0.0017175292386148208
Test RE1 dist truth: 0.00021469132288506158

###
plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.1__head__frozen_0 no backup
###
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.05__head__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.0375__head__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.025__head__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.015__head__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.008__head__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.004__head__frozen_0
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.2__head__frozen_0__seed_300 already trained
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.1__head__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.05__head__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.0375__head__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.025__head__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.015__head__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.008__head__frozen_0__seed_300
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.004__head__frozen_0__seed_300 already trained

###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: microsoft/resnet-152, augmentation: True, random crop: True, part: head, get_old_path: False, computer: 1,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.2, norm_mean: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), norm_std: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), sched: None seed: 500, 

train size: 2166, val size: 736, test size: 1064

##### CONTINUE TRAINING #####

Epoch 2701/4500: Training Loss: 0.2235188783314835		 Val Loss: 0.380942966627038
             	Training MAE: 0.06522326916456223		 Val MAE: 0.06112644076347351
             	Algebraic dist: 0.3512919802507351		 Val Algebraic dist: 0.5224745377250339
             	RE1 dist: 0.13196763605209294		 Val RE1 dist: 0.24723670793616254
             	SED dist: 0.41121304782994117		 Val SED dist: 0.7277482074239979

Epoch 2702/4500: Training Loss: 0.17085371686083803		 Val Loss: 0.5898924288542374
             	Training MAE: 0.06483788788318634		 Val MAE: 0.06162761151790619
             	Algebraic dist: 0.31254521331224056		 Val Algebraic dist: 0.6312861235245414
             	RE1 dist: 0.09591251809658599		 Val RE1 dist: 0.4541142505148183
             	SED dist: 0.30609862830806045		 Val SED dist: 1.1449896770974863

### Not decreasing ###

Epoch 2703/4500: Training Loss: 0.16413120677990228		 Val Loss: 0.26333858655846637
             	Training MAE: 0.06461164355278015		 Val MAE: 0.07015950232744217
             	Algebraic dist: 0.30078299547033555		 Val Algebraic dist: 0.3619830090066661
             	RE1 dist: 0.09119861504248587		 Val RE1 dist: 0.16797871175019638
             	SED dist: 0.29273671505635956		 Val SED dist: 0.48941089795983356

### Not decreasing ###

Epoch 2704/4500: Training Loss: 0.18379958881223335		 Val Loss: 0.21382296603658926
             	Training MAE: 0.06487952917814255		 Val MAE: 0.06227666884660721
             	Algebraic dist: 0.3215776844658095		 Val Algebraic dist: 0.3496274118838103
             	RE1 dist: 0.10400469452692573		 Val RE1 dist: 0.12563344706659732
             	SED dist: 0.3320067571098074		 Val SED dist: 0.3929999807606573

### Not decreasing ###

Epoch 1/10: Test SED dist: 0.34885561376585994
Epoch 2/10: Test SED dist: 0.33356464357304394
Epoch 3/10: Test SED dist: 0.3397012868321928
Epoch 4/10: Test SED dist: 0.3325516836983817
Epoch 5/10: Test SED dist: 0.34084064799143854
Epoch 6/10: Test SED dist: 0.35334382021337524
Epoch 7/10: Test SED dist: 0.35271640289995004
Epoch 8/10: Test SED dist: 0.34223321326693196
Epoch 9/10: Test SED dist: 0.3372821377632313
Epoch 10/10: Test SED dist: 0.33259505078308566


## TEST RESULTS: ##
Test Loss: 0.18795234995677057		 Test MAE: 0.06200527511537075
Test Algebraic dist: 0.32234433396418294
Test SED dist: 0.3413684500787491
Test RE1 dist: 0.11696975751030717

Test Algebraic dist truth: 0.017719037550732605
Test SED dist truth: 0.0017252970010714424
Test RE1 dist truth: 0.00021568531716676582

###
plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.2__head__frozen_0__seed_500 no backup
###
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.1__head__frozen_0__seed_500
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.05__head__frozen_0__seed_500 already trained
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.0375__head__frozen_0__seed_500
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.025__head__frozen_0__seed_500
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.015__head__frozen_0__seed_500 already trained
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.008__head__frozen_0__seed_500
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.004__head__frozen_0__seed_500
