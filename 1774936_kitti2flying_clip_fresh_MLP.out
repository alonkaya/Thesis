Thu 05 Dec 2024 21:40:07 IST

SLURM_JOBID:		 1774936
SLURM_JOB_NODELIST:	 ise-4090-16 


/home/alonkay/.conda/envs/alon_env/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/.conda/envs/alon_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/.conda/envs/alon_env/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
/cs_storage/alonkay/Thesis/FMatrixRegressor.py:331: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
###########################################################################################################################################################

__fresh_MLP learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Kitti2Flying,
average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, part: head, get_old_path: False,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 150, norm_mean: tensor([0.4815, 0.4578, 0.4082], device='cuda:0'), norm_std: tensor([0.2686, 0.2613, 0.2758], device='cuda:0'), sched: None seed: 42, 

train size: 1472, val size: 361, test size: 968

Traceback (most recent call last):
  File "/cs_storage/alonkay/Thesis/Main.py", line 110, in <module>
    model.train_model(train_loader, val_loader, test_loader)
  File "/cs_storage/alonkay/Thesis/FMatrixRegressor.py", line 191, in train_model
    self.dataloader_step(train_loader, epoch, epoch_stats, data_type="train")
  File "/cs_storage/alonkay/Thesis/FMatrixRegressor.py", line 277, in dataloader_step
    self.optimizer.step()
  File "/home/alonkay/.local/lib/python3.9/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/alonkay/.local/lib/python3.9/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/alonkay/.local/lib/python3.9/site-packages/torch/optim/adam.py", line 223, in step
    adam(
  File "/home/alonkay/.local/lib/python3.9/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
  File "/home/alonkay/.local/lib/python3.9/site-packages/torch/optim/adam.py", line 784, in adam
    func(
  File "/home/alonkay/.local/lib/python3.9/site-packages/torch/optim/adam.py", line 611, in _multi_tensor_adam
    exp_avg_sq_sqrt = torch._foreach_sqrt(device_exp_avg_sqs)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 8.56 MiB is free. Process 1681408 has 674.00 MiB memory in use. Process 2161155 has 762.00 MiB memory in use. Process 2164223 has 762.00 MiB memory in use. Process 2676267 has 992.00 MiB memory in use. Process 2903833 has 482.00 MiB memory in use. Process 2913604 has 472.00 MiB memory in use. Process 3150286 has 712.00 MiB memory in use. Process 3252975 has 472.00 MiB memory in use. Process 3282820 has 472.00 MiB memory in use. Process 528239 has 470.00 MiB memory in use. Process 532220 has 468.00 MiB memory in use. Process 534216 has 472.00 MiB memory in use. Process 536357 has 470.00 MiB memory in use. Process 538765 has 472.00 MiB memory in use. Process 542737 has 470.00 MiB memory in use. Process 557068 has 470.00 MiB memory in use. Process 564190 has 470.00 MiB memory in use. Process 568444 has 470.00 MiB memory in use. Process 520662 has 488.00 MiB memory in use. Process 521679 has 488.00 MiB memory in use. Process 522688 has 488.00 MiB memory in use. Process 524206 has 488.00 MiB memory in use. Process 527074 has 802.00 MiB memory in use. Including non-PyTorch memory, this process has 10.93 GiB memory in use. Of the allocated memory 10.25 GiB is allocated by PyTorch, and 182.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
