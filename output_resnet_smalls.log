nohup: ignoring input
/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

###
plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.025__head__frozen_0
Already trained and got bad results
###
/home/alonkay/Thesis/FMatrixRegressor.py:319: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
/home/alonkay/Thesis/FMatrixRegressor.py:322: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(path, "backup_model.pth"), map_location='cpu')

#########
using backup:
PytorchStreamReader failed locating file data.pkl: file not found
###########################################################################################################################################################

                         learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
                        batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a'], dataset: Stereo,
                        average embeddings: False, model: microsoft/resnet-152, augmentation: True, random crop: True, part: mid, get_old_path: False,
                        RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
                        crop: 224 resize: 256, use conv: False pretrained: None, train_size: 0.025, norm_mean: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), norm_std: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), sched: None seed: 42, 


##### CONTINUE TRAINING #####


Epoch 5010/12000: Training Loss: 0.43312731911154356		 Val Loss: 3.213637669881185
             	Training MAE: 0.06703145056962967		 Val MAE: 0.04802145063877106
             	Algebraic dist: 1.5279803556554459		 Val Algebraic dist: 3.7603893280029297
             	RE1 dist: 2.5727476232192097		 Val RE1 dist: 18.751185099283855
             	SED dist: 0.8083118550917682		 Val SED dist: 6.411693572998047


Epoch 5011/12000: Training Loss: 0.41670788035673256		 Val Loss: 2.7987953821818032
             	Training MAE: 0.06614227592945099		 Val MAE: 0.04429875686764717
             	Algebraic dist: 1.5312462974997127		 Val Algebraic dist: 3.6509663263956704
             	RE1 dist: 2.5809456320369946		 Val RE1 dist: 14.931994120279947
             	SED dist: 0.7773148031795726		 Val SED dist: 5.583400726318359


Epoch 5012/12000: Training Loss: 0.4483354512382956		 Val Loss: 2.7756640116373696
             	Training MAE: 0.06822807341814041		 Val MAE: 0.048616036772727966
             	Algebraic dist: 1.5973125906551586		 Val Algebraic dist: 3.5109904607137046
             	RE1 dist: 2.709358439725988		 Val RE1 dist: 16.1940434773763
             	SED dist: 0.8347451266120461		 Val SED dist: 5.533578872680664


