/home/alonkay/conda/alon/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/alonkay/conda/alon/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/alonkay/Thesis/FMatrixRegressor.py:338: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.004__tail__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.008__tail__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.1__tail__frozen_0
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.2__tail__frozen_0
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.004__tail__frozen_0__seed_300 already trained
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.008__tail__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.1__tail__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.2__tail__frozen_0__seed_300
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.004__tail__frozen_0__seed_500
no model for plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__Resnet__use_reconstruction_True/BS_8__ratio_0.008__tail__frozen_0__seed_500

###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: microsoft/resnet-152, augmentation: True, random crop: True, part: tail, get_old_path: False, computer: 1,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 0, trained vit: None,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.1, norm_mean: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), norm_std: tensor([0.5000, 0.5000, 0.5000], device='cuda:0'), sched: None seed: 500

train size: 1082, val size: 367, test size: 1064

##### CONTINUE TRAINING #####

Epoch 6511/7000: Training Loss: 0.12882114859188304		 Val Loss: 0.5291200720745585
             	Training MAE: 0.06376490741968155		 Val MAE: 0.0657840371131897
             	Algebraic dist: 0.2616923276115866		 Val Algebraic dist: 0.5684367884760317
             	RE1 dist: 0.06722470592049991		 Val RE1 dist: 0.31334916405055835
             	SED dist: 0.22329999418819652		 Val SED dist: 1.023464534593665

Epoch 6512/7000: Training Loss: 0.14155988132252412		 Val Loss: 1.3192666924518088
             	Training MAE: 0.06334444135427475		 Val MAE: 0.060805197805166245
             	Algebraic dist: 0.27390575408935547		 Val Algebraic dist: 1.0261549742325493
             	RE1 dist: 0.0750813694561229		 Val RE1 dist: 0.9349384307861328
             	SED dist: 0.248991377213422		 Val SED dist: 2.604298964790676

Epoch 6513/7000: Training Loss: 0.1533073256997501		 Val Loss: 0.4617302936056386
             	Training MAE: 0.06475315988063812		 Val MAE: 0.0597013458609581
             	Algebraic dist: 0.29088258743286133		 Val Algebraic dist: 0.4932853035304857
             	RE1 dist: 0.08237929905162138		 Val RE1 dist: 0.28336933384770935
             	SED dist: 0.27172425213982077		 Val SED dist: 0.890476309734842

Epoch 6514/7000: Training Loss: 0.21355056762695312		 Val Loss: 1.95928606779679
             	Training MAE: 0.06418807804584503		 Val MAE: 0.058742456138134
             	Algebraic dist: 0.35256287630866556		 Val Algebraic dist: 1.1214313507080078
             	RE1 dist: 0.1230163994957419		 Val RE1 dist: 1.392429020093835
             	SED dist: 0.3925936923307531		 Val SED dist: 3.8859607862389605

Epoch 6515/7000: Training Loss: 0.1340939016903148		 Val Loss: 2.9850706017535664
             	Training MAE: 0.06252580881118774		 Val MAE: 0.06087534874677658
             	Algebraic dist: 0.2689990997314453		 Val Algebraic dist: 1.497271496316661
             	RE1 dist: 0.07448955143199247		 Val RE1 dist: 2.1249348184336787
             	SED dist: 0.2344795255100026		 Val SED dist: 5.936839891516644

Epoch 6516/7000: Training Loss: 0.20423556776607737		 Val Loss: 0.6672728994618291
             	Training MAE: 0.06348936259746552		 Val MAE: 0.06482643634080887
             	Algebraic dist: 0.3345932960510254		 Val Algebraic dist: 0.661824309307596
             	RE1 dist: 0.11798280828139361		 Val RE1 dist: 0.41103209619936737
             	SED dist: 0.37432729496675377		 Val SED dist: 1.3000817506209663

Epoch 6517/7000: Training Loss: 0.1437064619625316		 Val Loss: 0.9623734018077021
             	Training MAE: 0.06458167731761932		 Val MAE: 0.059432677924633026
             	Algebraic dist: 0.28314629723044005		 Val Algebraic dist: 0.7501625392747961
             	RE1 dist: 0.07863837129929487		 Val RE1 dist: 0.6823630954908289
             	SED dist: 0.2526985617244945		 Val SED dist: 1.8915184684421704

Epoch 6518/7000: Training Loss: 0.1454760046566234		 Val Loss: 0.9401135651961617
             	Training MAE: 0.06316418945789337		 Val MAE: 0.0662117451429367
             	Algebraic dist: 0.28342047859640684		 Val Algebraic dist: 0.8078920944877293
             	RE1 dist: 0.079933201565462		 Val RE1 dist: 0.6091276666392451
             	SED dist: 0.25693071589750405		 Val SED dist: 1.8453816952912703

