nohup: ignoring input
/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/transformers/utils/generic.py:260: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.
  torch.utils._pytree._register_pytree_node(
/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/transformers/modeling_utils.py:479: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(checkpoint_file, map_location=map_location)
/home/aviran/Alon/Thesis/FMatrixRegressor.py:326: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(model_path, map_location='cpu')
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.025__head__frozen_0 already trained
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.025__head__frozen_4 already trained
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.025__mid__frozen_0 already trained
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.025__mid__frozen_4 already trained
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.025__tail__frozen_0 already trained
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.025__tail__frozen_4 already trained
Model plots/Stereo/Winners/SED_0.5__L2_1__huber_1__lr_0.0001__conv__CLIP__use_reconstruction_True/Trained_vit/BS_8__ratio_0.0375__head__frozen_0 already trained

#########
using backup:
PytorchStreamReader failed reading zip archive: failed finding central directory
/home/aviran/Alon/Thesis/FMatrixRegressor.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(os.path.join(path, "backup_model.pth"), map_location='cpu')
###########################################################################################################################################################

 learning rate: 0.0001, mlp_hidden_sizes: [1024, 512], jump_frames: 6, use_reconstruction_layer: True
batch_size: 8, norm: True, train_seqeunces: [0, 2, 3, 5], val_sequences: [6, 7, 8], RL_TEST_NAMES: ['fe2fadf89a84e92a', 'f01e8b6f8e10fdd9', 'f1ee9dc6135e5307', 'a41df4fa06fd391b', 'bc0ebb7482f14795', '9bdd34e784c04e3a', '98ebee1c36ecec55'], dataset: Stereo,
average embeddings: False, model: openai/clip-vit-base-patch32, augmentation: True, random crop: True, part: head, get_old_path: False,
RE1 coeff: 0 SED coeff: 0.5, ALG_COEFF: 0, L2_coeff: 1, huber_coeff: 1, frozen layers: 4, trained vit: plots/Affine/BS_32__lr_6e-05__train_size_9216__CLIP__alpha_10__conv__original_rotated/model.pth,
crop: 224 resize: 256, use conv: True pretrained: None, train_size: 0.0375, norm_mean: tensor([0.4815, 0.4578, 0.4082], device='cuda:0'), norm_std: tensor([0.2686, 0.2613, 0.2758], device='cuda:0'), sched: None seed: 42, 


##### CONTINUE TRAINING #####


Epoch 3060/10009: Training Loss: 0.5010609720267502		 Val Loss: 1.8370804256863065
             	Training MAE: 0.058737482875585556		 Val MAE: 0.04838540032505989
             	Algebraic dist: 1.2273677751129748		 Val Algebraic dist: 2.998067855834961
             	RE1 dist: 1.8329756493661917		 Val RE1 dist: 10.225325690375435
             	SED dist: 0.955118366316253		 Val SED dist: 3.657423655192057


Epoch 3061/10009: Training Loss: 0.5556474199482039		 Val Loss: 1.753703859117296
             	Training MAE: 0.060255520045757294		 Val MAE: 0.04243960231542587
             	Algebraic dist: 1.3178579293045343		 Val Algebraic dist: 3.0929701063368054
             	RE1 dist: 2.086395413267846		 Val RE1 dist: 8.753340827094185
             	SED dist: 1.0615458768956803		 Val SED dist: 3.4945237901475696


Epoch 3062/10009: Training Loss: 0.4421120063931334		 Val Loss: 1.5004121992323134
             	Training MAE: 0.05930576100945473		 Val MAE: 0.04858554154634476
             	Algebraic dist: 1.1245417875402115		 Val Algebraic dist: 2.6162272559271917
             	RE1 dist: 1.5786936142865349		 Val RE1 dist: 7.0731090969509545
             	SED dist: 0.835050994274663		 Val SED dist: 2.977787653605143


Epoch 3063/10009: Training Loss: 0.4826808630251417		 Val Loss: 1.4320665995279949
             	Training MAE: 0.05912495404481888		 Val MAE: 0.04607295244932175
             	Algebraic dist: 1.1861751780790442		 Val Algebraic dist: 2.6320298512776694
             	RE1 dist: 1.7372315350700827		 Val RE1 dist: 7.005993313259548
             	SED dist: 0.9164251439711627		 Val SED dist: 2.8459464179144964


Epoch 3064/10009: Training Loss: 0.4442252738803041		 Val Loss: 1.7018002404106989
             	Training MAE: 0.06015338748693466		 Val MAE: 0.04494548588991165
             	Algebraic dist: 1.1135125253714768		 Val Algebraic dist: 2.9211046430799694
             	RE1 dist: 1.547864278157552		 Val RE1 dist: 9.544781155056423
             	SED dist: 0.8383914723115808		 Val SED dist: 3.389550950792101


Epoch 3065/10009: Training Loss: 0.46227212045706956		 Val Loss: 1.645649062262641
             	Training MAE: 0.057208724319934845		 Val MAE: 0.04550466686487198
             	Algebraic dist: 1.1818287419337852		 Val Algebraic dist: 2.8560880025227866
             	RE1 dist: 1.690389446183747		 Val RE1 dist: 7.8530832926432295
             	SED dist: 0.8816826764275046		 Val SED dist: 3.276756074693468


Epoch 3066/10009: Training Loss: 0.45578724730248543		 Val Loss: 1.6581882900661893
             	Training MAE: 0.05582687258720398		 Val MAE: 0.05499246343970299
             	Algebraic dist: 1.1909502814797794		 Val Algebraic dist: 2.8336658477783203
             	RE1 dist: 1.7287884880514706		 Val RE1 dist: 7.424120585123698
             	SED dist: 0.8729178484748391		 Val SED dist: 3.287782245212131


Epoch 3067/10009: Training Loss: 0.4816872839834176		 Val Loss: 1.6539762285020616
             	Training MAE: 0.061514366418123245		 Val MAE: 0.04977109283208847
             	Algebraic dist: 1.1802766089345895		 Val Algebraic dist: 2.8382805718315973
             	RE1 dist: 1.6910681631050857		 Val RE1 dist: 8.851721021864149
             	SED dist: 0.9117685205796185		 Val SED dist: 3.2835333082411022


Epoch 3068/10009: Training Loss: 0.46877273858762253		 Val Loss: 1.4873857498168945
             	Training MAE: 0.058977123349905014		 Val MAE: 0.04505323991179466
             	Algebraic dist: 1.1745523938945694		 Val Algebraic dist: 2.6201568179660373
             	RE1 dist: 1.665317460602405		 Val RE1 dist: 6.780901590983073
             	SED dist: 0.8898655012542126		 Val SED dist: 2.960198084513346


Epoch 3069/10009: Training Loss: 0.45069679559445847		 Val Loss: 1.7400309244791667
             	Training MAE: 0.05924023315310478		 Val MAE: 0.04710027202963829
             	Algebraic dist: 1.1617038951200598		 Val Algebraic dist: 2.9811409844292536
             	RE1 dist: 1.6467572380514706		 Val RE1 dist: 9.261266920301649
             	SED dist: 0.8571606056362975		 Val SED dist: 3.4644752078586154


Epoch 3070/10009: Training Loss: 0.49420936434876683		 Val Loss: 1.8095421261257596
             	Training MAE: 0.05666918307542801		 Val MAE: 0.04501831904053688
             	Algebraic dist: 1.237908307243796		 Val Algebraic dist: 3.080249998304579
             	RE1 dist: 1.8336117314357383		 Val RE1 dist: 8.545955234103733
             	SED dist: 0.9467469757678462		 Val SED dist: 3.6047960917154946


Fatal Python error: Illegal instruction

Thread 0x0000783b4de00640 (most recent call first):
<no Python frame>

Current thread 0x0000783cce5ae740 (most recent call first):
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/serialization.py", line 886 in _save
  File "/home/aviran/miniconda3/envs/alon_env/lib/python3.9/site-packages/torch/serialization.py", line 652 in save
  File "/home/aviran/Alon/Thesis/FMatrixRegressor.py", line 285 in save_model
  File "/home/aviran/Alon/Thesis/FMatrixRegressor.py", line 229 in train_model
  File "/home/aviran/Alon/Thesis/Main.py", line 97 in <module>
